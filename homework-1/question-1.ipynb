{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Gradient Descent \n",
    "Consider the function:\n",
    "\n",
    "$$f(u,v,b)=-\\log⁡\\sigma(u+b)-\\log⁡\\sigma(v+b)-\\log⁡\\sigma\\bigg(- {u + v + 2b \\over 2}\\bigg)+{u^2+v^2+b^2\\over 100}$$\n",
    "\n",
    "where $u,v,b \\in \\mathbb{R}$ and \n",
    "\n",
    "$$\\sigma(x) = {1\\over 1+e^{-x}} = {e^{x} \\over e^{x} + 1} $$\n",
    "\n",
    "is the sigmoid function. We will encounter objective functions like this one later in a more complex way when we discuss neural networks. The objective function here is actually the one of logistic regression for three data points with $L_2$​-regularization. You might have learned about logistic regression in another course such as data analytics for engineers. \n",
    "\n",
    "We try to find the minimum of $f$ with the gradient descent algorithm. In particular, we evaluate various step-size policies. In order to do this, you need implement the following:\n",
    "- Implement a function that takes a point $(u,v,b)$ and returns the the gradient of $f$ at this point.</li>\n",
    "- Implement a function `gradient_descent(f, grad_f, eta, (u_0, v_0, b_0), max_iter=100)` that performs `max_iter` gradient descent steps $$x_{t + 1} \\leftarrow x_t - \\eta(t) \\cdot \\nabla f(x_t​)$$ where $f$ is the function to be minimized, $\\nabla f$ returns the gradient (implemented by `grad_f`), $\\eta(t)$ returns the step-size at iteration $t$ (implemented by `eta`) and $(u_0,v_0,b_0)$ is the starting point (initialization).\n",
    "\n",
    "Using these functions, perform $100$ gradient descent steps, starting at $(u_0,v_0,b_0) = (4,2,1)$ and return the function value of $f(u_{100},v_{100}, b_{100})$ and the lowest (best) function value achieved throughout the $100$ steps for the step size policies below."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Definition of the gradient of $f$\n",
    "for $f(u,v,b) = - \\log(\\frac{e^{u+b}}{e^{u+b}+1}) - \\log(\\frac{e^{v+b}}{e^{v+b}+1}) -log(\\frac{e^{- \\frac{u+v+2b}{2}}}{e^{- \\frac{u+v+2b}{2}}+1}) +\\frac{u^2 + v^2 + b^2}{100}$, we can compute the gradient of $f$ at a point $(u,v,b)$ as follows:\n",
    "- $\\frac{\\partial f}{\\partial u} = - \\frac{1}{e^{u+b}+1} + \\frac{1}{2e^{- \\frac{u+v+2b}{2}}+2} + \\frac{u}{50}$\n",
    "- $\\frac{\\partial f}{\\partial v} = - \\frac{1}{e^{v+b}+1} + \\frac{1}{2e^{- \\frac{u+v+2b}{2}}+2} + \\frac{v}{50}$\n",
    "- $\\frac{\\partial f}{\\partial b} = - \\frac{1}{e^{u+b}+1} - \\frac{1}{e^{v+b}+1} + \\frac{1}{e^{- \\frac{u+v+2b}{2}}+1} + \\frac{b}{50}$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\nabla f(u,v,b)=\\left[\\begin{array}{c}\n",
    "\\dfrac{\\partial f}{\\partial u}(v,b)\\\\\n",
    "\\dfrac{\\partial f}{\\partial v}(u,b) \\\\\n",
    "\\dfrac{\\partial f}{\\partial b}(u,v) \n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "- \\dfrac{1}{e^{u+b}+1} + \\dfrac{1}{2e^{- \\frac{u+v+2b}{2}}+2} + \\dfrac{u}{50} \\\\\n",
    "- \\dfrac{1}{e^{v+b}+1} + \\dfrac{1}{2e^{- \\frac{u+v+2b}{2}}+2} + \\dfrac{v}{50} \\\\\n",
    "- \\dfrac{1}{e^{u+b}+1} - \\dfrac{1}{e^{v+b}+1} + \\dfrac{1}{e^{- \\frac{u+v+2b}{2}}+1} + \\dfrac{b}{50}\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.026187Z",
     "start_time": "2024-11-16T15:00:44.831710Z"
    }
   },
   "source": [
    "# implementation of function returning the gradient of f at a point (u,v,b)\n",
    "import numpy as np\n",
    "def grad_f(u, v, b):\n",
    "    # gradient of f at (u,v,b)\n",
    "    grad_u = -1/(np.exp(u+b)+1) + 1/(2*np.exp(-(u+v+2*b)/2)+2) + u/50\n",
    "    grad_v = -1/(np.exp(v+b)+1) + 1/(2*np.exp(-(u+v+2*b)/2)+2) + v/50\n",
    "    grad_b = -1/(np.exp(u+b)+1) - 1/(np.exp(v+b)+1) + 1/(np.exp(-(u+v+2*b)/2)+1) + b/50\n",
    "    return grad_u, grad_v, grad_b"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Definition of the gradient descent algorithm\n",
    "The gradient descent algorithm is defined as follows:\n",
    "- Input: function $f$, gradient of $f$ `grad_f`, step size $\\eta$, starting point $(u_0,v_0,b_0)$, maximum number of iterations `max_iter`\n",
    "- Output: $f(u_{100},v_{100},b_{100})$ and $f_{\\text{best}} = \\min_{1\\leq t \\leq 100} f(u_t,v_t,b_t)$\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.039997Z",
     "start_time": "2024-11-16T15:00:45.033206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# implementation of the gradient descent algorithm\n",
    "def gradient_descent(f, grad_f, eta, x0, max_iter=100):\n",
    "    # initialization\n",
    "    x = np.array(x0)\n",
    "    f_best = f(*x)\n",
    "    # iterate\n",
    "    for t in range(max_iter):\n",
    "        # compute gradient\n",
    "        grad = np.array(grad_f(*x))\n",
    "        # update x\n",
    "        x = x - eta(t) * grad\n",
    "        # update f_best\n",
    "        f_best = min(f_best, f(*x))\n",
    "    return f(*x), f_best"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1a (8 points)\n",
    "Use a constant step size strategy: implement a function `eta_const( t, c=0.2 )` that returns for each iteration `t` the constant `c` as step size. Using this step-size policy, what are the results? "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.240154Z",
     "start_time": "2024-11-16T15:00:45.234299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# implementation of the constant step size strategy\n",
    "def eta_const(t, c=0.2):\n",
    "    return c"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the final function value after $100$ iterations? So $f(u_{100}, v_{100}, b_{100}) =$"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.288432Z",
     "start_time": "2024-11-16T15:00:45.262331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# perform gradient descent with constant step size\n",
    "u0, v0, b0 = 4, 2, 1\n",
    "x0 = (u0, v0, b0)\n",
    "f = lambda u, v, b: -np.log(1/(np.exp(u+b)+1)) - np.log(1/(np.exp(v+b)+1)) - np.log(1/(np.exp(-(u+v+2*b)/2)+1) ) + (u**2+v**2+b**2)/100\n",
    "\n",
    "f_final, f_best = gradient_descent(f, grad_f, eta_const, x0)\n",
    "f_final"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6297032372655194"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best function value obtained throughout the training process? So $\\displaystyle\\min _{1\\leq t \\leq 100}f(u_t,v_t,b_t) =$"
   ]
  },
  {
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.356955Z",
     "start_time": "2024-11-16T15:00:45.349779Z"
    }
   },
   "cell_type": "code",
   "source": "f_best",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6297032372655194"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1b (6 points)\n",
    "Use a continuously decreasing step size strategy: implement a function `eta_sqrt( t, c=0.5 )` that returns for iteration $t$ the step size $c\\over\\sqrt{t+1}$​. Using this step size policy, what are the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.392020Z",
     "start_time": "2024-11-16T15:00:45.386587Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the final function value after $100$ iterations? So $f(u100,v100,b100) =$\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.422828Z",
     "start_time": "2024-11-16T15:00:45.415096Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "What is the best function value obtained throughout the training process? $\\displaystyle\\min _{1\\leq t \\leq 100} f(u_t,v_t,b_t)=$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.494678Z",
     "start_time": "2024-11-16T15:00:45.492238Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1c (6 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a multi-step step-size strategy: implement a function `eta_multistep( t, milestones=[30,50,80], c=0.5, eta_init=1.0 )` that returns a step-size that is initially set to `eta_init`, but is decayed at each milestone by multiplying it with factor `c`. For example:\n",
    "\n",
    "$$\\texttt{eta\\_multistep( t, [20,50], c=0.1, eta\\_init=1 )} = \\begin{cases}\n",
    "1    & \\text{if } t < 20 \\\\\n",
    "0.1  & \\text{if } 20 \\geq t < 50 \\\\\n",
    "0.01 & \\text{if } 50 \\geq t\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.576883Z",
     "start_time": "2024-11-16T15:00:45.570141Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the final function value after 100 iterations? So $f (w_{100}, b_{100}​) =$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.636358Z",
     "start_time": "2024-11-16T15:00:45.631464Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best function value obtained throughout the training process? $\\displaystyle\\min _{1\\leq t \\leq 100} f(w_t,b_t) =$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T15:00:45.671449Z",
     "start_time": "2024-11-16T15:00:45.668547Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Gradient Descent \n",
    "Consider the function:\n",
    "\n",
    "$$f(u,v,b)=-\\log⁡\\sigma(u+b)-\\log⁡\\sigma(v+b)-\\log⁡\\sigma\\bigg(- {u + v + 2b \\over 2}\\bigg)+{u^2+v^2+b^2\\over 100}$$\n",
    "\n",
    "where $u,v,b \\in \\mathbb{R}$ and \n",
    "\n",
    "$$\\sigma(x) = {1\\over 1+e^{-x}} = {e^{x} \\over e^{x} + 1} $$\n",
    "\n",
    "is the sigmoid function. We will encounter objective functions like this one later in a more complex way when we discuss neural networks. The objective function here is actually the one of logistic regression for three data points with $L_2$​-regularization. You might have learned about logistic regression in another course such as data analytics for engineers. \n",
    "\n",
    "We try to find the minimum of ff with the gradient descent algorithm. In particular, we evaluate various step-size policies. In order to do this, you need implement the following:\n",
    "- Implement a function that takes a point (u,v,b)(u,v,b) and returns the the gradient of ff at this point.</li>\n",
    "- Implement a function `gradient_descent(f, grad_f, eta, (u_0, v_0, b_0), max_iter=100)` that performs `max_iter` gradient descent steps $$x_{t + 1} \\leftarrow x_t - \\eta(t) \\cdot \\nabla f(x_t​)$$ where $f$ is the function to be minimized, $\\nabla f$ returns the gradient (implemented by `grad_f`), $\\eta(t)$ returns the step-size at iteration $t$ (implemented by `eta`) and $(u_0,v_0,b_0)$ is the starting point (initialization).\n",
    "\n",
    "Using these functions, perform $100$ gradient descent steps, starting at $(u_0,v_0,b_0) = (4,2,1)$ and return the function value of $f(u_{100},v_{100}, b_{100})$ and the lowest (best) function value achieved throughout the $100$ steps for the step size policies below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1a (8 points)\n",
    "Use a constant step size strategy: implement a function `eta_const( t, c=0.2 )` that returns for each iteration `t` the constant `c` as step size. Using this step-size policy, what are the results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the final function value after $100$ iterations? So $f(u_{100}, v_{100}, b_{100}) =$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best function value obtained throughout the training process? So $\\displaystyle\\min _{1\\leq t \\leq 100}f(u_t,v_t,b_t) =$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1b (6 points)\n",
    "Use a continuously decreasing step size strategy: implement a function `eta_sqrt( t, c=0.5 )` that returns for iteration $t$ the step size $c\\over\\sqrt{t+1}$​. Using this step size policy, what are the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the final function value after $100$ iterations? So $f(u100,v100,b100) =$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "What is the best function value obtained throughout the training process? $\\displaystyle\\min _{1\\leq t \\leq 100} f(u_t,v_t,b_t)=$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1c (6 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a multi-step step-size strategy: implement a function `eta_multistep( t, milestones=[30,50,80], c=0.5, eta_init=1.0 )` that returns a step-size that is initially set to `eta_init`, but is decayed at each milestone by multiplying it with factor `c`. For example:\n",
    "\n",
    "$$\\texttt{eta\\_multistep( t, [20,50], c=0.1, eta\\_init=1 )} = \\begin{cases}\n",
    "1    & \\text{if } t < 20 \\\\\n",
    "0.1  & \\text{if } 20 \\geq t < 50 \\\\\n",
    "0.01 & \\text{if } 50 \\geq t\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the final function value after 100 iterations? So $f (w_{100}, b_{100}​) =$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best function value obtained throughout the training process? $\\displaystyle\\min _{1\\leq t \\leq 100} f(w_t,b_t) =$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

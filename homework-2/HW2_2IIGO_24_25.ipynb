{
 "cells": [
  {
   "cell_type": "code",
   "id": "19e52de0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:30:46.612668Z",
     "start_time": "2024-12-17T14:30:46.608312Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # data visualization library"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "fb541e6c",
   "metadata": {},
   "source": "If possible, update your sklearn version to 1.5.2 to reduce variance in the versions."
  },
  {
   "cell_type": "code",
   "id": "fef750e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:30:48.745580Z",
     "start_time": "2024-12-17T14:30:48.726514Z"
    }
   },
   "source": [
    "# The scikit-learn version is 1.5.2.\n",
    "\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.5.2.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# The numpy version should be 2.1.3.",
   "id": "560d8c49dba0f25f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:31:11.849087Z",
     "start_time": "2024-12-17T14:31:11.843330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sklearn\n",
    "print('The numpy version is {}.'.format(np.__version__))"
   ],
   "id": "d646b5c6e0f67e74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numpy version is 2.1.3.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "099b0d20",
   "metadata": {},
   "source": [
    "# PCA Faces"
   ]
  },
  {
   "cell_type": "code",
   "id": "ce5b245c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:24:53.740422Z",
     "start_time": "2024-12-17T14:24:53.427849Z"
    }
   },
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "faces = fetch_olivetti_faces()\n",
    "print(faces.DESCR)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _olivetti_faces_dataset:\n",
      "\n",
      "The Olivetti faces dataset\n",
      "--------------------------\n",
      "\n",
      "`This dataset contains a set of face images`_ taken between April 1992 and\n",
      "April 1994 at AT&T Laboratories Cambridge. The\n",
      ":func:`sklearn.datasets.fetch_olivetti_faces` function is the data\n",
      "fetching / caching function that downloads the data\n",
      "archive from AT&T.\n",
      "\n",
      ".. _This dataset contains a set of face images: https://cam-orl.co.uk/facedatabase.html\n",
      "\n",
      "As described on the original website:\n",
      "\n",
      "    There are ten different images of each of 40 distinct subjects. For some\n",
      "    subjects, the images were taken at different times, varying the lighting,\n",
      "    facial expressions (open / closed eyes, smiling / not smiling) and facial\n",
      "    details (glasses / no glasses). All the images were taken against a dark\n",
      "    homogeneous background with the subjects in an upright, frontal position\n",
      "    (with tolerance for some side movement).\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "=================   =====================\n",
      "Classes                                40\n",
      "Samples total                         400\n",
      "Dimensionality                       4096\n",
      "Features            real, between 0 and 1\n",
      "=================   =====================\n",
      "\n",
      "The image is quantized to 256 grey levels and stored as unsigned 8-bit\n",
      "integers; the loader will convert these to floating point values on the\n",
      "interval [0, 1], which are easier to work with for many algorithms.\n",
      "\n",
      "The \"target\" for this database is an integer from 0 to 39 indicating the\n",
      "identity of the person pictured; however, with only 10 examples per class, this\n",
      "relatively small dataset is more interesting from an unsupervised or\n",
      "semi-supervised perspective.\n",
      "\n",
      "The original dataset consisted of 92 x 112, while the version available here\n",
      "consists of 64x64 images.\n",
      "\n",
      "When using these images, please give credit to AT&T Laboratories Cambridge.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deff78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you plot the first thirty pictures of the dataset\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "for i in range(30):\n",
    "    ax = plt.subplot2grid((3, 10), (int(i/10), i-int(i/10)*10))\n",
    "    \n",
    "    ax.imshow(faces.data[i].reshape(64, 64), cmap=plt.cm.gray)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = faces.data\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315546a",
   "metadata": {},
   "source": "Use the function `linalg.svd` from `scipy (version 1.14.1)` to compute the PCA for exercise 1a, b, c. The PCs can always point into two directions: the positive and the negative. To reproduce the result and to get the same image as provided as an option you need to use scipy. If you use numpy, you get however a similarly looking face from which you probably can infer the correct solution in ANS. Note: Using a different library may yield different results, trying to use direct PCA function from a library will not do the job here.  "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "# .. = linalg.svd(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf726b",
   "metadata": {},
   "source": [
    "# k-means initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 show numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c8ccc",
   "metadata": {},
   "source": [
    "If your versions don't match, the following commands (or their anaconda version) could help to get the newest stable release. If you need help with this, please ask the TAs during instruction hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12878ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174c2de",
   "metadata": {},
   "source": [
    "The functions generating the datasets are given here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63937a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMoons(epsilon, n):\n",
    "    moons, labels = sklearn.datasets.make_moons(n_samples=n, noise=epsilon, random_state=7)\n",
    "    return \"moons\", moons, labels, 2\n",
    "def generateBlobs(epsilon, n):\n",
    "    blobs, labels = sklearn.datasets.make_blobs(n_samples=n,centers=3, cluster_std=[epsilon + 1, epsilon + 1.5, epsilon + 0.5], random_state=54)\n",
    "    return \"blobs\", blobs, labels, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa617152",
   "metadata": {},
   "source": [
    "Implement the centroid initialization here. Right now, it returns a random initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_centroids_greedy_pp(D,r,l=10):\n",
    "    '''\n",
    "        :param r: (int) number of centroids (clusters)\n",
    "        :param D: (np-array) the data matrix\n",
    "        :param l: (int) number of centroid candidates in each step\n",
    "        :return: (np-array) 'X' the selected centroids from the dataset\n",
    "    '''   \n",
    "    rng =  np.random.default_rng(seed=7) # use this random generator to sample the candidates (sampling according to given probabilities can be done via rng.choice(..))\n",
    "    n,d = D.shape\n",
    "\n",
    "    indexes = rng.integers(low=0, high=n, size=r)\n",
    "    X = np.array(D[indexes,:]).T\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def spectral_clustering(W,r, X_init):\n",
    "    '''\n",
    "        :param W: (np-array) nxn similarity/weighted adjacency matrix\n",
    "        :param r: (int) number of centroids (clusters)\n",
    "        :param X_init: (function) the centroid initialization function \n",
    "        :return: (np-array) 'Y' the computed cluster assignment matrix\n",
    "    '''  \n",
    "    np.random.seed(0)\n",
    "    L = np.diag(np.array(W.sum(0))[0]) - W\n",
    "    v0 = np.random.rand(min(L.shape))\n",
    "    Lambda, V = scipy.sparse.linalg.eigsh(L, k=r+1, which=\"SM\", v0=v0)\n",
    "    A = V[:,1:] #remove the first eigenvector, assuming that the graph is conected\n",
    "    initial_points = X_init(A,r)\n",
    "    X, Y = kmeans(A, r, initial_points)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947969f",
   "metadata": {},
   "source": [
    "This is the $k$-means implementation from the lecture accompanying notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSS(D,X,Y):\n",
    "    return np.sum((D- Y@X.T)**2)\n",
    "def getY(labels):\n",
    "    '''\n",
    "        Compute the cluster assignment matrix Y from the categorically encoded labels\n",
    "    '''\n",
    "    Y = np.eye(max(labels)+1)[labels]\n",
    "    return Y\n",
    "def update_centroid(D,Y):\n",
    "    cluster_sizes = np.diag(Y.T@Y).copy()\n",
    "    cluster_sizes[cluster_sizes==0]=1\n",
    "    return D.T@Y/cluster_sizes\n",
    "def update_assignment(D,X):\n",
    "    dist = np.sum((np.expand_dims(D,2) - X)**2,1)\n",
    "    labels = np.argmin(dist,1)\n",
    "    return getY(labels)\n",
    "def kmeans(D,r, X_init, epsilon=0.00001, t_max=10000):\n",
    "    X = X_init.copy()\n",
    "    Y = update_assignment(D,X)\n",
    "    rss_old = RSS(D,X,Y) +2*epsilon\n",
    "    t=0\n",
    "    #Looping as long as difference of objective function values is larger than epsilon\n",
    "    while rss_old - RSS(D,X,Y) > epsilon and t < t_max-1:\n",
    "        rss_old = RSS(D,X,Y)\n",
    "        X = update_centroid(D,Y)\n",
    "        Y = update_assignment(D,X)\n",
    "        t+=1\n",
    "    print(t,\"iterations\")\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e2027",
   "metadata": {},
   "source": [
    "## Running k-means on the blobs dataset\n",
    "We generate the blobs dataset. Run kmeans based on the initialization technique. Finally, we plot the clustering. The initial centroids are marked in red, and the final centroids are marked in blue. You can use this visualization to see if your initialization makes sense. It doesn't work for spectral clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de116bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=500\n",
    "dataID, D, labels, r = generateBlobs(0.05,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_init = init_centroids_greedy_pp(D,r)\n",
    "X,Y = kmeans(D,r, X_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e810b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.axis('equal')\n",
    "ax.scatter(D[:, 0], D[:, 1], c=np.argmax(Y,axis=1), s=10)\n",
    "ax.scatter(X_init.T[:, 0], X_init.T[:, 1], c='red', s=50, marker = 'D') # initial centroids are in red\n",
    "ax.scatter(X.T[:, 0], X.T[:, 1], c='blue', s=50, marker = 'D') # computed centroids are in blue\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b4cd2",
   "metadata": {},
   "source": [
    "## Running spectral clustering on the two moons dataset\n",
    "We generate the moons dataset and compute spectral clustering with the implemented initialization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ede9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataID, D, labels, r = generateMoons(0.05,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ac20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import radius_neighbors_graph, kneighbors_graph\n",
    "# Implement here the computation of W as knn graph\n",
    "W = radius_neighbors_graph(D,0.5,include_self=False)\n",
    "Y = spectral_clustering(W,r,init_centroids_greedy_pp)\n",
    "\n",
    "plt.scatter(D[:, 0], D[:, 1], c=np.argmax(Y,axis=1), s=10)\n",
    "plt.title('%s'  % ( dataID) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3017c5",
   "metadata": {},
   "source": [
    "# Your own personal Netflix\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024678f8",
   "metadata": {},
   "source": [
    "To read the dataset you might need to alter the path to look for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce05e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas is a data manipulation library\n",
    "# lets explore movies.csv\n",
    "movies= pd.read_csv('ml-latest-small/movies.csv')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets explore ratings.CSV\n",
    "ratings=pd.read_csv('ml-latest-small/ratings.csv',sep=',')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146a856",
   "metadata": {},
   "source": [
    "The given ratings are in the range of 0.5 and 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(ratings[\"rating\"]), max(ratings[\"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da2825",
   "metadata": {},
   "source": [
    "We convert the sparse data representation of movie ratings into a data matrix. The missing values are filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed04f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie_ratings = ratings.pivot(\n",
    "    index='userId',\n",
    "    columns='movieId',\n",
    "    values='rating'\n",
    ").fillna(0)  #fill unobserved entries with μ\n",
    "df_movie_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769cbf1",
   "metadata": {},
   "source": [
    "We consider here only the movies which have been rated by more than 100 users. That are 134 movies. We will not be able to infer a pattern for movies with very few observations anyways, but for this exercise we are mostly interested in the principle and do not need a big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc403d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.sum(df_movie_ratings!=0,0)>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceeeb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_movie = np.sum(df_movie_ratings!=0,0)>100\n",
    "df_D = df_movie_ratings.loc[:,keep_movie]\n",
    "df_D.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f7375",
   "metadata": {},
   "source": [
    "Furthermore, we will throw out all the users which have rated fewer than five movies. It would be hard anyways to make recommendations based on 4 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfe764",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.sum(df_D!=0,1)>=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dde580",
   "metadata": {},
   "source": [
    "The resulting dataset has the userID as rows and movieIDs as columns. Hence, userID 1 and 4 addresses the first two rows of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_user = np.sum(df_D!=0,1)>=5\n",
    "df_D = df_D.loc[keep_user,:]\n",
    "df_D.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e496d3",
   "metadata": {},
   "source": [
    "The movie number- title assignments are given as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc[movies['movieId'].isin(df_D.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64258b18",
   "metadata": {},
   "source": [
    "The resulting data matrix is given as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ed4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = df_D.to_numpy()\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f831f5",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Use the following initialization for your implementation of the optimization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c66d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_completion(D,r, t_max=100, λ = 0.1):\n",
    "    np.random.seed(0)\n",
    "    X = np.random.normal(size =(d,r))\n",
    "    Y = np.random.normal(size =(n,r))\n",
    "    # Implement now the optimization procedure\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1bbb1",
   "metadata": {},
   "source": [
    "# MNIST ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76926cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605acbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters to play with\n",
    "dropout_1 = 0.9\n",
    "dropout_2 = 0.9\n",
    "lr = 0.9\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac56ef",
   "metadata": {},
   "source": [
    "## Define the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# The embedding architecture returns the \n",
    "# output of the penultimate layer\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_size:int, out_size:int, kernel_size:int, groups=1):\n",
    "        super().__init__()\n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_size, out_size, kernel_size, padding=\"same\", groups=groups)\n",
    "        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=\"same\", groups=groups)\n",
    "        self.skip_connection = nn.Identity() if in_size==out_size else nn.Conv2d(in_size, out_size, kernel_size=1, padding=\"same\")\n",
    "        # normalization layers\n",
    "        self.norm1 = nn.BatchNorm2d(in_size)\n",
    "        self.norm2 = nn.BatchNorm2d(out_size)\n",
    "        # activation functions\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor)->torch.Tensor:\n",
    "        h = self.conv1(self.act1(self.norm1(x)))\n",
    "        h = self.conv2(self.act2(self.norm2(h)))\n",
    "        x = self.skip_connection(x)\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, embedding_dim:int, dropout1:float=dropout_1, dropout2:float=dropout_2):\n",
    "        super(Embed, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 32, kernel_size=5, padding=\"same\")\n",
    "        # res blocks\n",
    "        self.block1 = ResBlock(32, 32, 3, groups=2)\n",
    "        self.block2 = ResBlock(32, 32, 3, groups=2)\n",
    "        self.block3 = ResBlock(32, 64, 3, groups=4)\n",
    "        self.block4 = ResBlock(64, 64, 3, groups=4)\n",
    "        # pooling layer\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # normalization layers\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "        # linear layers\n",
    "        self.fc1   = nn.Linear(64, 128)\n",
    "        self.fc2   = nn.Linear(128, embedding_dim)\n",
    "        # dropout\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        out = F.relu(self.norm1(self.conv(x)))   # convolution to prepare for res blocks\n",
    "        # out.shape = ? (tensor shape 1)\n",
    "        out = self.block2(self.block1(out))  # first set of res blocks\n",
    "        # out.shape = ? (tensor shape 2)\n",
    "        out = self.norm2(self.pool(out))  # pooling\n",
    "        # out.shape = ? (tensor shape 3)\n",
    "        out = self.block4(self.block3(out))  # second set of res blocks\n",
    "        # out.shape = ? (tensor shape 4)\n",
    "        out = torch.mean(out, dim=(-1, -2))  # average over spatial dimensions\n",
    "        out = self.norm3(out)  # batch norm before fully connected part\n",
    "        # out.shape = ? (tensor shape 5)\n",
    "        \n",
    "        # fully connected part:\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out     \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self,embedding_dim, classifier):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed = Embed(embedding_dim=embedding_dim)\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def conf(self,x):\n",
    "        out = self.embed(x)\n",
    "        return F.softmax(self.classifier(out),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f4a49",
   "metadata": {},
   "source": [
    "## Load the data: the first four MNIST classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08393ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#classes = ('0', '1', '2', '3', '4', '5')\n",
    "c=10\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=trans)\n",
    "\n",
    "# Select only some classes \n",
    "idx = train_data.targets < c\n",
    "train_data.targets = train_data.targets[idx]\n",
    "train_data.data = train_data.data[idx]\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=trans)\n",
    "# Select only some classes \n",
    "idx = testset.targets < c\n",
    "testset.targets = testset.targets[idx]\n",
    "testset.data = testset.data[idx]\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704aca2",
   "metadata": {},
   "source": [
    "## Implementation of the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667479e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch(net, criterion, optimizer, trainloader, verbose=False):\n",
    "    train_loss, correct, conf = 0, 0, 0\n",
    "    start_time=time.time()\n",
    "    net.train() \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Set the gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Do the forward pass\n",
    "        logits = net(inputs)\n",
    "        loss = criterion(logits, targets)\n",
    "        if verbose:\n",
    "            print(\"loss:\",loss.item())\n",
    "        # Do the backward pass\n",
    "        loss.backward()\n",
    "        # Do a gradient descent step\n",
    "        optimizer.step()\n",
    "    \n",
    "        with torch.no_grad(): #Disable gradient tracking and compute some statistics\n",
    "            train_loss += loss.item()\n",
    "            y_probs = F.softmax(logits, dim=1)\n",
    "            confBatch, predicted = y_probs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            conf+=confBatch.sum().item()\n",
    "    execution_time = (time.time() - start_time)\n",
    "    n=len(trainloader.dataset)\n",
    "    print('Loss: %.3f | Acc: %.3f%% (%d/%d) | Conf %.2f | time (s): %.2f'% (train_loss/len(trainloader), 100.*correct/n, correct, n, 100*conf/n, execution_time))\n",
    "    return (100.*correct/n, 100*conf/n)\n",
    "  \n",
    "def test_acc(net, criterion, data_loader):\n",
    "    net.eval()\n",
    "    test_loss, correct, conf, total = 0,0,0,0\n",
    "    with torch.no_grad(): # disable gradient tracking\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = net(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            y_probs = F.softmax(logits, dim=1)\n",
    "            confBatch, predicted = y_probs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            conf+=confBatch.sum().item()\n",
    "    n=len(data_loader.dataset)\n",
    "    print('Loss: %.3f | Acc: %.3f%% (%d/%d) | Conf %.2f'% (test_loss/max(len(data_loader),1), 100.*correct/n, correct, n, 100*conf/n))\n",
    "    return (100.*correct/n, 100*conf/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de0f21",
   "metadata": {},
   "source": [
    "## Create the model and perform the optimization for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=2\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "classifier = nn.Linear(d, c, bias=True)\n",
    "net = Net(embedding_dim=d, classifier=classifier)\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "sgd = torch.optim.SGD([{'params': net.parameters()},],\n",
    "                lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    train_epoch(net, criterion, sgd, trainloader)\n",
    "    (acc,conf) = test_acc(net,criterion, testloader)\n",
    "\n",
    "print('Saving..')\n",
    "state = {'net': net.state_dict(),'acc': acc}\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "torch.save(state, './checkpoint/net.t7')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88402898",
   "metadata": {},
   "source": [
    "## Plot the latent space representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved net\n",
    "classifier = nn.Linear(d, c,bias=True)\n",
    "net = Net(embedding_dim=d, classifier=classifier)\n",
    "checkpoint = torch.load(\"checkpoint/net.t7\",map_location='cpu')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.eval()\n",
    "print('ACC:\\t',checkpoint['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "def scatter_pictures(inputs, outputs, samples=30):\n",
    "    zoom = 0.7\n",
    "        \n",
    "    for j in range(min(inputs.shape[0],samples)):\n",
    "        image = inputs[j,:,:,:].squeeze()\n",
    "        im = OffsetImage(image, cmap=\"gray\",zoom=zoom)\n",
    "        ab = AnnotationBbox(im, (outputs[j,0], outputs[j,1]), xycoords='data', frameon=False, alpha=0.5)\n",
    "        ax.add_artist(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03caf847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_conf(conf, show_class_assignment=False, x_max=20, y_max=20, x_min=-1, y_min=-1):\n",
    "    x = np.arange(x_min, x_max, 0.05)\n",
    "    y = np.arange(y_min, y_max, 0.05)\n",
    "\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    X = np.array([xx,yy]).reshape(2,x.shape[0]*y.shape[0]).T\n",
    "    Z = conf(torch.from_numpy(X).float()).t()\n",
    "    Z = Z.reshape(-1,y.shape[0],x.shape[0]).cpu().detach().numpy()\n",
    "    if show_class_assignment:\n",
    "        h = plt.contourf(x,y,Z.argmax(axis=0),cmap='magma')\n",
    "    else:\n",
    "        h = plt.contourf(x,y,Z.max(axis=0),cmap='magma')\n",
    "        plt.clim(0, 1)\n",
    "        cb = plt.colorbar()\n",
    "        cb.set_label('Confidence')\n",
    "    plt.axis('scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(testloader)) #load a batch\n",
    "outputs = net.embed(inputs).detach()\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plot_conf((lambda x: torch.softmax(net.classifier(x),dim=1)), x_max =max(outputs[:,0])+5, y_max =max(outputs[:,1])+5, x_min =min(outputs[:,0])-3, y_min =min(outputs[:,1])-3)\n",
    "scatter_pictures(inputs, outputs,samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b172a",
   "metadata": {},
   "source": [
    "## Plot Representations of out-of-distribution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "c=10\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=trans)\n",
    "trainloader_fashion = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=trans)\n",
    "testloader_fashion = torch.utils.data.DataLoader(testset, batch_size=512, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(testloader_fashion))\n",
    "outputs = net.embed(inputs).detach()\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plot_conf((lambda x: torch.softmax(net.classifier(x),dim=1)), x_max =max(outputs[:,0])+5, y_max =max(outputs[:,1])+5, x_min =min(outputs[:,0])-3, y_min =min(outputs[:,1])-3)\n",
    "scatter_pictures(inputs, outputs,samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234a3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

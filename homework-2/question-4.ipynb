{
 "cells": [
  {
   "cell_type": "code",
   "id": "19e52de0",
   "metadata": {
    "id": "19e52de0",
    "ExecuteTime": {
     "end_time": "2025-01-09T09:59:47.274328Z",
     "start_time": "2025-01-09T09:59:46.665259Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # data visualization library"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "fb541e6c",
   "metadata": {
    "id": "fb541e6c"
   },
   "source": [
    "If possible, update your sklearn version to 1.5.2 to reduce variance in the versions."
   ]
  },
  {
   "cell_type": "code",
   "id": "fef750e0",
   "metadata": {
    "id": "fef750e0",
    "outputId": "b02f1b17-e450-4195-9617-f97355191fd5",
    "ExecuteTime": {
     "end_time": "2025-01-09T09:59:49.064353Z",
     "start_time": "2025-01-09T09:59:47.290702Z"
    }
   },
   "source": [
    "# The scikit-learn version is 1.5.2.\n",
    "\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.5.2.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "d646b5c6e0f67e74",
    "outputId": "ae4e9f42-f399-4c4b-f41d-157f53d9aee2",
    "ExecuteTime": {
     "end_time": "2025-01-09T09:59:49.299778Z",
     "start_time": "2025-01-09T09:59:49.293989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sklearn\n",
    "print('The numpy version is {}.'.format(np.__version__))"
   ],
   "id": "d646b5c6e0f67e74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numpy version is 2.2.1.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "27d1bbb1",
   "metadata": {
    "id": "27d1bbb1"
   },
   "source": [
    "# MNIST ANN"
   ]
  },
  {
   "cell_type": "code",
   "id": "76926cd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "76926cd7",
    "outputId": "d178692c-bc91-4795-bf3e-225d058fbfb6",
    "ExecuteTime": {
     "end_time": "2025-01-09T09:59:51.662531Z",
     "start_time": "2025-01-09T09:59:49.377511Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "605acbf2",
   "metadata": {
    "id": "605acbf2",
    "ExecuteTime": {
     "end_time": "2025-01-09T09:59:51.716379Z",
     "start_time": "2025-01-09T09:59:51.711915Z"
    }
   },
   "source": [
    "# hyper parameters to play with\n",
    "dropout_1 = 0.1\n",
    "dropout_2 = 0.1\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "batch_size = 128\n",
    "epochs = 5"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "8aac56ef",
   "metadata": {
    "id": "8aac56ef"
   },
   "source": [
    "## Define the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "id": "359b27f9",
   "metadata": {
    "id": "359b27f9",
    "ExecuteTime": {
     "end_time": "2025-01-09T09:59:51.746390Z",
     "start_time": "2025-01-09T09:59:51.734545Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "HAS_PRINTED = False\n",
    "\n",
    "# The embedding architecture returns the\n",
    "# output of the penultimate layer\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_size:int, out_size:int, kernel_size:int, groups=1):\n",
    "        super().__init__()\n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_size, out_size, kernel_size, padding=\"same\", groups=groups)\n",
    "        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=\"same\", groups=groups)\n",
    "        self.skip_connection = nn.Identity() if in_size==out_size else nn.Conv2d(in_size, out_size, kernel_size=1, padding=\"same\")\n",
    "        # normalization layers\n",
    "        self.norm1 = nn.BatchNorm2d(in_size)\n",
    "        self.norm2 = nn.BatchNorm2d(out_size)\n",
    "        # activation functions\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor)->torch.Tensor:\n",
    "        h = self.conv1(self.act1(self.norm1(x)))\n",
    "        h = self.conv2(self.act2(self.norm2(h)))\n",
    "        x = self.skip_connection(x)\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, embedding_dim:int, dropout1:float=dropout_1, dropout2:float=dropout_2):\n",
    "        super(Embed, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 32, kernel_size=5, padding=\"same\")\n",
    "        # res blocks\n",
    "        self.block1 = ResBlock(32, 32, 3, groups=2)\n",
    "        self.block2 = ResBlock(32, 32, 3, groups=2)\n",
    "        self.block3 = ResBlock(32, 64, 3, groups=4)\n",
    "        self.block4 = ResBlock(64, 64, 3, groups=4)\n",
    "        # pooling layer\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # normalization layers\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "        # linear layers\n",
    "        self.fc1   = nn.Linear(64, 128)\n",
    "        self.fc2   = nn.Linear(128, embedding_dim)\n",
    "        # dropout\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        global HAS_PRINTED\n",
    "        out = F.relu(self.norm1(self.conv(x)))   # convolution to prepare for res blocks\n",
    "        if not HAS_PRINTED:\n",
    "          print(f\"tensor shape 1: {out.shape=}\")\n",
    "        out = self.block2(self.block1(out))  # first set of res blocks\n",
    "        if not HAS_PRINTED:\n",
    "          print(f\"tensor shape 2: {out.shape=}\")\n",
    "        out = self.norm2(self.pool(out))  # pooling\n",
    "        if not HAS_PRINTED:\n",
    "          print(f\"tensor shape 3: {out.shape=}\")\n",
    "        out = self.block4(self.block3(out))  # second set of res blocks\n",
    "        if not HAS_PRINTED:\n",
    "          print(f\"tensor shape 4: {out.shape=}\")\n",
    "        out = torch.mean(out, dim=(-1, -2))  # average over spatial dimensions\n",
    "        out = self.norm3(out)  # batch norm before fully connected part\n",
    "        if not HAS_PRINTED:\n",
    "          print(f\"tensor shape 5: {out.shape=}\")\n",
    "\n",
    "        HAS_PRINTED = True\n",
    "\n",
    "        # fully connected part:\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,embedding_dim, classifier):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed = Embed(embedding_dim=embedding_dim)\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def conf(self,x):\n",
    "        out = self.embed(x)\n",
    "        return F.softmax(self.classifier(out),dim=1)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "c31f4a49",
   "metadata": {
    "id": "c31f4a49"
   },
   "source": [
    "## Load the data: the first four MNIST classes"
   ]
  },
  {
   "cell_type": "code",
   "id": "08393ddc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08393ddc",
    "outputId": "564210be-c20e-4357-9bd0-6a44906eb511",
    "ExecuteTime": {
     "end_time": "2025-01-09T10:00:28.208282Z",
     "start_time": "2025-01-09T09:59:51.756274Z"
    }
   },
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#classes = ('0', '1', '2', '3', '4', '5')\n",
    "c=10\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=trans)\n",
    "\n",
    "# Select only some classes\n",
    "idx = train_data.targets < c\n",
    "train_data.targets = train_data.targets[idx]\n",
    "train_data.data = train_data.data[idx]\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=trans)\n",
    "# Select only some classes\n",
    "idx = testset.targets < c\n",
    "testset.targets = testset.targets[idx]\n",
    "testset.data = testset.data[idx]\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:13<00:00, 761kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 313kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.53MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "4704aca2",
   "metadata": {
    "id": "4704aca2"
   },
   "source": [
    "## Implementation of the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "id": "667479e7",
   "metadata": {
    "id": "667479e7",
    "ExecuteTime": {
     "end_time": "2025-01-09T10:00:28.243717Z",
     "start_time": "2025-01-09T10:00:28.234319Z"
    }
   },
   "source": [
    "import time\n",
    "def train_epoch(net, criterion, optimizer, trainloader, verbose=False):\n",
    "    train_loss, correct, conf = 0, 0, 0\n",
    "    start_time=time.time()\n",
    "    net.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Set the gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Do the forward pass\n",
    "        logits = net(inputs)\n",
    "        loss = criterion(logits, targets)\n",
    "        if verbose:\n",
    "            print(\"loss:\",loss.item())\n",
    "        # Do the backward pass\n",
    "        loss.backward()\n",
    "        # Do a gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad(): #Disable gradient tracking and compute some statistics\n",
    "            train_loss += loss.item()\n",
    "            y_probs = F.softmax(logits, dim=1)\n",
    "            confBatch, predicted = y_probs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            conf+=confBatch.sum().item()\n",
    "    execution_time = (time.time() - start_time)\n",
    "    n=len(trainloader.dataset)\n",
    "    print('Loss: %.3f | Acc: %.3f%% (%d/%d) | Conf %.2f | time (s): %.2f'% (train_loss/len(trainloader), 100.*correct/n, correct, n, 100*conf/n, execution_time))\n",
    "    return (100.*correct/n, 100*conf/n)\n",
    "\n",
    "def test_acc(net, criterion, data_loader):\n",
    "    net.eval()\n",
    "    test_loss, correct, conf, total = 0,0,0,0\n",
    "    with torch.no_grad(): # disable gradient tracking\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = net(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            y_probs = F.softmax(logits, dim=1)\n",
    "            confBatch, predicted = y_probs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            conf+=confBatch.sum().item()\n",
    "    n=len(data_loader.dataset)\n",
    "    print('Loss: %.3f | Acc: %.3f%% (%d/%d) | Conf %.2f'% (test_loss/max(len(data_loader),1), 100.*correct/n, correct, n, 100*conf/n))\n",
    "    return (100.*correct/n, 100*conf/n)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "a9de0f21",
   "metadata": {
    "id": "a9de0f21"
   },
   "source": [
    "## Create the model and perform the optimization for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "id": "c371b034",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c371b034",
    "outputId": "5b975f9a-d370-4ce0-be71-2a425c7e2c6f",
    "ExecuteTime": {
     "end_time": "2025-01-09T10:00:28.290543Z",
     "start_time": "2025-01-09T10:00:28.261004Z"
    }
   },
   "source": [
    "d=2\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "classifier = nn.Linear(d, c, bias=True)\n",
    "net = Net(embedding_dim=d, classifier=classifier)\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "sgd = torch.optim.SGD([{'params': net.parameters()},],\n",
    "                lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "e08f253d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e08f253d",
    "outputId": "11f42036-d716-48c4-d272-25616350d873",
    "ExecuteTime": {
     "end_time": "2025-01-09T10:10:59.125081Z",
     "start_time": "2025-01-09T10:00:28.306126Z"
    }
   },
   "source": [
    "import os\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    train_epoch(net, criterion, sgd, trainloader)\n",
    "    (acc,conf) = test_acc(net,criterion, testloader)\n",
    "\n",
    "print('Saving..')\n",
    "state = {'net': net.state_dict(),'acc': acc}\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "torch.save(state, './checkpoint/net.t7')\n",
    ""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "tensor shape 1: out.shape=torch.Size([128, 32, 28, 28])\n",
      "tensor shape 2: out.shape=torch.Size([128, 32, 28, 28])\n",
      "tensor shape 3: out.shape=torch.Size([128, 32, 14, 14])\n",
      "tensor shape 4: out.shape=torch.Size([128, 64, 14, 14])\n",
      "tensor shape 5: out.shape=torch.Size([128, 64])\n",
      "Loss: 0.860 | Acc: 71.410% (42846/60000) | Conf 61.72 | time (s): 132.89\n",
      "Loss: 1.271 | Acc: 60.670% (6067/10000) | Conf 75.88\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 0.325 | Acc: 93.188% (55913/60000) | Conf 86.25 | time (s): 125.21\n",
      "Loss: 0.266 | Acc: 93.640% (9364/10000) | Conf 90.16\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 0.227 | Acc: 95.305% (57183/60000) | Conf 90.88 | time (s): 131.90\n",
      "Loss: 0.361 | Acc: 91.360% (9136/10000) | Conf 85.18\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 0.177 | Acc: 96.317% (57790/60000) | Conf 92.87 | time (s): 128.08\n",
      "Loss: 0.146 | Acc: 97.090% (9709/10000) | Conf 94.16\n",
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m epoch)\n\u001B[1;32m----> 4\u001B[0m     \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msgd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     (acc,conf) \u001B[38;5;241m=\u001B[39m test_acc(net,criterion, testloader)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSaving..\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[8], line 11\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(net, criterion, optimizer, trainloader, verbose)\u001B[0m\n\u001B[0;32m      9\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Do the forward pass\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(logits, targets)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n",
      "File \u001B[1;32m~\\Documents\\TUe\\BCS - Y4\\Q2 Courses\\2IIG0 Data Mining and Machine Learning\\Homework\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\TUe\\BCS - Y4\\Q2 Courses\\2IIG0 Data Mining and Machine Learning\\Homework\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[6], line 90\u001B[0m, in \u001B[0;36mNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 90\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(out)\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\Documents\\TUe\\BCS - Y4\\Q2 Courses\\2IIG0 Data Mining and Machine Learning\\Homework\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\TUe\\BCS - Y4\\Q2 Courses\\2IIG0 Data Mining and Machine Learning\\Homework\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[6], line 55\u001B[0m, in \u001B[0;36mEmbed.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x:torch\u001B[38;5;241m.\u001B[39mTensor)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39mtorch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mglobal\u001B[39;00m HAS_PRINTED\n\u001B[1;32m---> 55\u001B[0m     out \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)   \u001B[38;5;66;03m# convolution to prepare for res blocks\u001B[39;00m\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m HAS_PRINTED:\n\u001B[0;32m     57\u001B[0m       \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensor shape 1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mout\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\TUe\\BCS - Y4\\Q2 Courses\\2IIG0 Data Mining and Machine Learning\\Homework\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\TUe\\BCS - Y4\\Q2 Courses\\2IIG0 Data Mining and Machine Learning\\Homework\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Documents\\TUe\\BCS - Y4\\Q2 Courses\\2IIG0 Data Mining and Machine Learning\\Homework\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    186\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    188\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[0;32m    196\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\TUe\\BCS - Y4\\Q2 Courses\\2IIG0 Data Mining and Machine Learning\\Homework\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2812\u001B[0m, in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2809\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[0;32m   2810\u001B[0m     _verify_batch_size(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m-> 2812\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2813\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2814\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2815\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2816\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrunning_mean\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2817\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrunning_var\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2818\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2819\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2820\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2821\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2822\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "88402898",
   "metadata": {
    "id": "88402898"
   },
   "source": [
    "## Plot the latent space representations"
   ]
  },
  {
   "cell_type": "code",
   "id": "97be45ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97be45ed",
    "outputId": "fc0a0933-14fe-4dc2-f581-e4ebc4f4ad29"
   },
   "source": [
    "# Load the saved net\n",
    "classifier = nn.Linear(d, c,bias=True)\n",
    "net = Net(embedding_dim=d, classifier=classifier)\n",
    "checkpoint = torch.load(\"checkpoint/net.t7\",map_location='cpu')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.eval()\n",
    "print('ACC:\\t',checkpoint['acc'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a02c22f",
   "metadata": {
    "id": "8a02c22f"
   },
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "def scatter_pictures(inputs, outputs, samples=30):\n",
    "    zoom = 0.7\n",
    "\n",
    "    for j in range(min(inputs.shape[0],samples)):\n",
    "        image = inputs[j,:,:,:].squeeze()\n",
    "        im = OffsetImage(image, cmap=\"gray\",zoom=zoom)\n",
    "        ab = AnnotationBbox(im, (outputs[j,0], outputs[j,1]), xycoords='data', frameon=False, alpha=0.5)\n",
    "        ax.add_artist(ab)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "03caf847",
   "metadata": {
    "id": "03caf847"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_conf(conf, show_class_assignment=False, x_max=20, y_max=20, x_min=-1, y_min=-1):\n",
    "    x = np.arange(x_min, x_max, 0.05)\n",
    "    y = np.arange(y_min, y_max, 0.05)\n",
    "\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    X = np.array([xx,yy]).reshape(2,x.shape[0]*y.shape[0]).T\n",
    "    Z = conf(torch.from_numpy(X).float()).t()\n",
    "    Z = Z.reshape(-1,y.shape[0],x.shape[0]).cpu().detach().numpy()\n",
    "    if show_class_assignment:\n",
    "        h = plt.contourf(x,y,Z.argmax(axis=0),cmap='magma')\n",
    "    else:\n",
    "        h = plt.contourf(x,y,Z.max(axis=0),cmap='magma')\n",
    "        plt.clim(0, 1)\n",
    "        cb = plt.colorbar()\n",
    "        cb.set_label('Confidence')\n",
    "    plt.axis('scaled')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "205a7562",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "205a7562",
    "outputId": "0fcd946e-5fb1-4bc4-821e-05365b2e14db"
   },
   "source": [
    "inputs, targets = next(iter(testloader)) #load a batch\n",
    "outputs = net.embed(inputs).detach()\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plot_conf((lambda x: torch.softmax(net.classifier(x),dim=1)), x_max =max(outputs[:,0])+5, y_max =max(outputs[:,1])+5, x_min =min(outputs[:,0])-3, y_min =min(outputs[:,1])-3)\n",
    "scatter_pictures(inputs, outputs,samples=100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "248b172a",
   "metadata": {
    "id": "248b172a"
   },
   "source": [
    "## Plot Representations of out-of-distribution data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec7e0556",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec7e0556",
    "outputId": "d05f4770-e946-4ebf-f0a8-7a60ecafaeab"
   },
   "source": [
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "c=10\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=trans)\n",
    "trainloader_fashion = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=trans)\n",
    "testloader_fashion = torch.utils.data.DataLoader(testset, batch_size=512, shuffle=False, num_workers=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c216bfd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "c216bfd9",
    "outputId": "eccfec79-708c-4ac0-9d64-6ba5edadce00"
   },
   "source": [
    "inputs, targets = next(iter(testloader_fashion))\n",
    "outputs = net.embed(inputs).detach()\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plot_conf((lambda x: torch.softmax(net.classifier(x),dim=1)), x_max =max(outputs[:,0])+5, y_max =max(outputs[:,1])+5, x_min =min(outputs[:,0])-3, y_min =min(outputs[:,1])-3)\n",
    "scatter_pictures(inputs, outputs,samples=100)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
